# default config for everything

# batch size for ptb
batch_size: 128
# batch size for wiki2
# batch_size: 8

# ptb is 9900, wiki103 is 226800, wiki2 is 28785
# n_vocab: 28785
n_vocab: 9900

epochs: 121
n_hidden_G: 512
n_layers_G: 2
n_hidden_E: 512
n_layers_E: 1
n_z: 100
word_dropout: 0.0
rec_coef: 7
lr: 0.0001
n_highway_layers: 2

unk_token: '<unk>'
pad_token: '<pad>'
start_token: '<sos>'
end_token: '<eos>'

# seeding
seed: 12345

# pretrained embedding, make sure these three match
n_embed: 300
vector: '6B'
vocab_vector: 'glove.6B.300d'

# CMA stuff
num_workers: 1
budget: 500
optim_filename: 'data/cma/optim.pkl'
npz_filename: 'data/cma/log.npz'